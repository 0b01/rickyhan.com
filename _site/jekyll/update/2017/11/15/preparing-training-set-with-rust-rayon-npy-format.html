<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Gradient Trader Part 4: Preparing Training Set with Rust, Rayon and npy binary format</title>
  <meta name="description" content="Preparing dataset for machine learning is a CPU heavy task. To optimize for GPU utilization during training, it is imperative to preprocess the data. What is...">


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98721585-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-98721585-1');
  </script>

  
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://rickyhan.com/jekyll/update/2017/11/15/preparing-training-set-with-rust-rayon-npy-format.html">
  <link rel="alternate" type="application/rss+xml" title="Ricky Han blog" href="http://rickyhan.com/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Ricky Han blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Gradient Trader Part 4: Preparing Training Set with Rust, Rayon and npy binary format</h1>
    <p class="post-meta"><time datetime="2017-11-15T23:00:00-05:00" itemprop="datePublished">Nov 15, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Preparing dataset for machine learning is a CPU heavy task. To optimize for GPU utilization during training, it is imperative to preprocess the data. What is the proper way to approach this? This depends on how much data you have.</p>

<p>I recently encountered this problem while training a new model. I had 60GB <a href="http://rickyhan.com/jekyll/update/2017/10/27/how-to-handle-order-book-data.html">compressed</a> order book data with which I need to generate a 680GB dataset.</p>

<p>This post is about the awkward situation where the dataset is not big enough to warrant Spark but would take too long to run on your computer. A top-end deep learning box only has a maximum of 32 cores while AWS has 128 cores on demand for $13.388/hr. A simple back-of-the-envelop calculation shows that if a task takes 24 hours on an Intel i7 with 8 threads, or <code class="highlighter-rouge">24 * 8 (hour x thread)</code> then it would only take ~1 hour to run on a 128 core instance for the price of a burrito. Another pro is the huge memory(1952GB) that should fit most datasets.</p>

<p>I use Rust to do the heavy lifting and in this post I will cover these two aspects:</p>

<ol>
  <li>
    <p>Using multiple cores</p>
  </li>
  <li>
    <p>Saving to Numpy format</p>
  </li>
</ol>

<h1 id="parallel-programming">Parallel Programming</h1>

<p>I suck at writing parallel code. About 4 months ago, I wrote this monstrocity. Feel free to skip it.</p>

<p>```python
import multiprocessing
import tensorflow as tf
from functools import partial
import gc</p>

<p>def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</p>

<p>batch_size = 256
time_steps = 256
max_scale = max(SCALES)
min_padding = max_scale * time_steps  # of minutes must be padded in front and back
maximum_sequential_epoch_sequence = range(get_min_epoch(), get_max_epoch(), 60)
ok_epochs = set(maximum_sequential_epoch_sequence[min_padding:-min_padding])
# ok_epochs = set(list(ok_epochs)[:len(ok_epochs)/8])
# ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8   : len(ok_epochs)/8<em>2])
# ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8</em>2 : len(ok_epochs)/8<em>3])
# ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8</em>3 : len(ok_epochs)/8*4])</p>

<h1 id="okepochs--setlistokepochslenokepochs84--lenokepochs85">ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8<em>4 : len(ok_epochs)/8</em>5])</h1>
<p># ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8<em>5 : len(ok_epochs)/8</em>6])
# ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8<em>6 : len(ok_epochs)/8</em>7])
# ok_epochs = set(list(ok_epochs)[len(ok_epochs)/8*7 : ])</p>

<p>def gen_all_the_tf_records(n, myepochs):
    tfrecords_filename = ‘/home/ubuntu/tfrecords/5/{}.tfrecords’.format(n)
    writer = tf.python_io.TFRecordWriter(tfrecords_filename)
    for random_epochs in tqdm(myepochs):
        features = [redacted]
        minibatch = np.stack(features, axis=0)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    example = tf.train.Example(features=tf.train.Features(feature={
        'minibatch': _bytes_feature(minibatch.tostring())
    }))
    writer.write(example.SerializeToString())
    gc.collect()
writer.close()
</code></pre>
</div>

<p>def tfrecords_cli():
    global ok_epochs
    iterable = range(len(ok_epochs) / batch_size)
    arrs = []
    threads = 35
    for _ in range(threads):
        arrs.append([])</p>

<div class="highlighter-rouge"><pre class="highlight"><code>for i, _ in tqdm(enumerate(iterable)):
    for arr_i in range(threads):
        if batch_size &gt; len(ok_epochs):
            break
        rand = random.sample(ok_epochs, batch_size)
        arrs[arr_i].append(rand)
        ok_epochs -= set(rand)
gc.collect()
ps = []
for i in range(threads):
    p = multiprocessing.Process(
        target=gen_all_the_tf_records, args=(i, arrs[i]))
    ps.append(p)
for i, p in enumerate(ps):
    print i , "started"
    p.start() ```
</code></pre>
</div>

<p>Notice how the <code class="highlighter-rouge">ok_epochs</code> are changed during every run because it just wouldn’t fit into memory. This piece of crap worked and I don’t want to talk about it.</p>

<p>Compare it to Rayon, parallelization requires virtually no change. This example is taken from <a href="http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/">here</a>.</p>

<p><code class="highlighter-rouge">rust
// sequential
let total_price = stores.iter()
                        .map(|store| store.compute_price(&amp;list))
                        .sum();
// parallel
let total_price = stores.into_par_iter()
                        .map(|store| store.compute_price(&amp;list))
                        .sum();
</code></p>

<p>When I tested Rayon on my laptop, it <em>really</em> was 4x as fast. This is about as easy it gets when it comes to multi-core programming.</p>

<h1 id="using-numpy-format">Using Numpy format</h1>

<p>I save minibatches in <a href="https://docs.scipy.org/doc/numpy-1.13.0/neps/npy-format.html">numpy binary format</a>.</p>

<p>To restore a numpy array from disk:</p>

<p><code class="highlighter-rouge">
import numpy as np
minibatch = np.load("minibatch.npy")
</code></p>

<p>Again, minimal coding required and no tfrecords involved.</p>

<p>Since I’m dealing with spatial-temporal data(for RNN), I need to generate feature tensor of shape <code class="highlighter-rouge">[batch_size, time_step, input_dim]</code>. To do this, I wrote a serializer for the <code class="highlighter-rouge">npy</code> format.</p>

<p>```rust
// write_npy.rs
use byteorder::{BE, LE, WriteBytesExt};
use std::io::Write;</p>

<p>use record::*;</p>

<p>static MAGIC_VALUE : &amp;[u8] = &amp;[0x93, 0x4E, 0x55, 0x4D, 0x50, 0x59];</p>

<p>fn get_header() -&gt; String {</p>

<div class="highlighter-rouge"><pre class="highlight"><code>format!("{{'descr': [('data', '&gt;f4')],'fortran_order': False,'shape': ({},{},{})}}",
    BATCH_SIZE, TIME_STEP, INPUT_DIM)
</code></pre>
</div>

<p>}</p>

<p>/// these are just from the spec
pub fn write(wtr: &amp;mut Write, record: &amp;Record) {
    let _ = wtr.write(MAGIC_VALUE);
    let _ = wtr.write_u8(0x01); // major version
    let _ = wtr.write_u8(0x00); // minor version
    let header = &amp;get_header();
    let header_len = header.len();
    let _ = wtr.write_u16::<le>(header_len as u16);
    let _ = wtr.write(header.as_bytes()); // header</le></p>

<div class="highlighter-rouge"><pre class="highlight"><code>for batch in record.iter() {
    for step in batch.iter() {
        for input in step.iter() {
            let _ = wtr.write_f32::&lt;BE&gt;(*input);
        }
    }
} } ```
</code></pre>
</div>

<p>```rust
// record.rs
pub const INPUT_DIM: usize = 6;
pub const TIME_STEP: usize = 5;
pub const BATCH_SIZE: usize = 4;</p>

<p>// shape [batch_size, time_step, input_dim]
pub type Record = [[[ f32 ; INPUT_DIM]; TIME_STEP]; BATCH_SIZE];
```</p>

<p>```rust
// main.rs</p>

<p>extern crate byteorder;
mod write_npy;
mod record;</p>

<p>use record::*;
use std::io::BufWriter;
use std::fs::File;</p>

<p>fn main() {
    let fname = “minibatch.npy”;
    let new_file = File::create(fname).unwrap();
    let mut wtr = BufWriter::new(new_file);</p>

<div class="highlighter-rouge"><pre class="highlight"><code>let mut record = [[[ 0_f32 ; INPUT_DIM]; TIME_STEP]; BATCH_SIZE];
for batch in 0..BATCH_SIZE {
    for step in 0..TIME_STEP {
        for dim in 0..INPUT_DIM {
            record[batch][step][dim] = (100 * batch + 10 * step + 1* dim) as f32;
        }
    }
}

write_npy::write(&amp;mut wtr, &amp;record); } ```
</code></pre>
</div>

<p>This is a straightforward implementation of the serializer. Now we can load some of these numpy files with PyToch:</p>

<p>```python
class orderbookDataset(torch.utils.Dataset):
    def <strong>init</strong>(self):
        self.data_files = os.listdir(‘data_dir’)
        sort(self.data_files)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>def __getindex__(self, idx):
    return np.load(self.data_files[idx])

def __len__(self):
    return len(self.data_files)
</code></pre>
</div>

<p>dset = OrderbookDataset()
loader = torch.utils.DataLoader(dset, num_workers=8)
```</p>

<h2 id="conclusion">Conclusion</h2>

<p>Using Rust to prepare training set is as easy as it get.</p>

<h1 id="if-you-find-this-article-helpful-consider-signing-up-to-my-email-listhttpstinylettercomrickyhan"><a href="https://tinyletter.com/rickyhan">If you find this article helpful, consider signing up to my email list</a></h1>

<form style="border:1px solid #ccc;padding:3px;text-align:center;" action="https://tinyletter.com/rickyhan" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/rickyhan', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true"><p><label for="tlemail">Enter your email address</label></p><p><input type="text" style="width:140px" name="email" id="tlemail" /></p><input type="hidden" value="1" name="embed" /><input type="submit" value="Subscribe" /><p><a href="https://tinyletter.com" target="_blank">powered by TinyLetter</a></p></form>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Ricky Han blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Ricky Han blog</li>
          <li><a href="mailto:rickyhan+blog@rickyhan.com">rickyhan+blog@rickyhan.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Programming demos, tips, thoughts.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
