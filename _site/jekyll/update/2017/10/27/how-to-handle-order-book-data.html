<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Gradient Trader Part 3: Efficient Storage of Order Book Data</title>
  <meta name="description" content="Dealing with order book data is a problem everyone encounters at some point. The more professional a firm is, the more severe the problem gets.">


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98721585-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-98721585-1');
  </script>

  
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://rickyhan.com/jekyll/update/2017/10/27/how-to-handle-order-book-data.html">
  <link rel="alternate" type="application/rss+xml" title="Ricky Han blog" href="http://rickyhan.com/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Ricky Han blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
          <a class="page-link" href="/cribdrag/">Crib Dragging</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Gradient Trader Part 3: Efficient Storage of Order Book Data</h1>
    <p class="post-meta"><time datetime="2017-10-27T22:37:01-04:00" itemprop="datePublished">Oct 27, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Dealing with order book data is a problem everyone encounters at some point. The more professional a firm is, the more severe the problem gets.</p>

<p>As documented in a <a href="http://rickyhan.com/jekyll/update/2017/09/09/import-orderbook-from-exchanges.html">previous post</a>, I have been storing order book ticks to a PostgreSQL. Using an out-of-the-box solution seems like a good idea at first, but it stops working before you notice. After amassing a whopping 2 terabytes of data, the system gradually slowed down. In the meantime, I have been working on a datastore specifically for storing order book updates. It uses a compressed binary file and stores one row in 12 bytes. This post is a short overview of how it is implemented. At its current stage, it is a filesystem backed append-only storage. The complexity is nowhere near SecDB, Athena, Quartz. In the future, I plan on adding clustering, master-slave replication, implement a better DSL, events, and <a href="https://medium.com/@istanbul_techie/a-look-at-conflict-free-replicated-data-types-crdt-221a5f629e7e">CRDT</a> should the demand arise.</p>

<h2 id="dense-tick-format-dtf--thinking-about-compression">Dense Tick Format (.dtf) : thinking about compression</h2>

<p>First of all, structure data can be compressed. The key is finding a sweet spot between processing speed and storage efficiency. If you have read my previous posts, you will know that the contiguous data is of this shape:</p>

<p><code class="highlighter-rouge">json
{
    "ts": 509129207.487,
    "seq": 79683,
    "is_trade": false,
    "is_bid": false,
    "price": 0.00540787,
    "size": 1.2227914
}
</code></p>

<p>These 6 fields are the minimal amount of information to reconstruct the order book in its entirety. (<a href="http://rickyhan.com/jekyll/update/2017/09/24/visualizing-order-book.html">Note on reconstruction</a>)</p>

<h3 id="naive-approach">Naive approach</h3>

<p>The naive approach is to use a CSV file. Storing it in plaintext (like txt) would require 45 bytes. Text file is neither efficient in storage nor in processing speed. We will compare our codecs against this baseline.</p>

<h3 id="our-first-codec">Our first codec</h3>

<p>Let’s do the bare minimum. Switching from ascii (1 byte per char), we use default data types to encode the same information:</p>

<p><code class="highlighter-rouge">
ts (u32): int
seq (u32): int
is_trade: (u8): bool
is_bid: (u8): bool
price: (f32): float
size: (f32): float
</code></p>

<p>Timestamp is stored as an unsigned integer by multiplying the float by 1000.</p>

<p>Sums up to <code class="highlighter-rouge">4 + 4 + 1 + 1 + 4 + 4 = 18 bytes</code>, a 60% reduction. This approach also takes significantly less time because string parsing is unnecessary. However, there are still plenty of low hanging fruits.</p>

<h3 id="bitflags">Bitflags</h3>

<p>Note how the bool is stored as a whole byte when it only takes 1 bit. This is because a byte is the smallest addressable unit in memory. We can squish the two bools into 1 byte by using bitflags. Now we have 17 bytes, a 5% reduction.</p>

<p><code class="highlighter-rouge">
ts (u32)
seq (u32)
((is_trade &lt;&lt; 1) | is_bid): (u8)
price: (f32)
size: (f32)
</code></p>

<p>To do this, I used the <code class="highlighter-rouge">bitflags</code> crate in Rust.</p>

<p>```rust
// use bitflags macro to define a struct
bitflags! {
    struct Flags: u8 {
        const FLAG_EMPTY   = 0b0000_0000;
        const FLAG_IS_BID   = 0b0000_0001;
        const FLAG_IS_TRADE = 0b0000_0010;
    }
}</p>

<p>let mut flags = Flags::FLAG_EMPTY;
if self.is_bid { flags |= Flags::FLAG_IS_BID; }
if self.is_trade { flags |= Flags::FLAG_IS_TRADE; }
let _ = buf.write_u8(flags.bits());
```</p>

<h3 id="delta-encoding">Delta encoding</h3>

<p>Instead of storing each tick as its own row, we can exploit the shared structure along the time axis, namely taking snapshot of time and sequence number. Since <code class="highlighter-rouge">timestamp</code> and <code class="highlighter-rouge">seq</code> are discretely increasing fields, we can create a snapshot the usual size (int) and store the difference in a smaller data type(short). This method is called delta encoding.</p>

<p><code class="highlighter-rouge">
0. bool for is_snapshot
1. if snapshot
    4 bytes (u32): reference ts
    2 bytes (u32): reference seq
    2 bytes (u16): how many records between this snapshot and the next snapshot
2. record
    dts (u16): $ts - reference ts$, 2^16 = 65536 ms = ~65 seconds
    dseq (u8) $seq - reference seq$ , 2^8 = 256
    (is_trade &lt;&lt; 1) | is_bid`: (u8): bitwise and to store two bools in one byte
    price: (f32)
    size: (f32)
</code></p>

<p>The idea is to save a snapshot every once in a while, and everytime when <code class="highlighter-rouge">dts</code> and <code class="highlighter-rouge">dseq</code> are close to overflow, start a new snapshot and repeat. This on average saves 5 bytes, a 30% reduction. And the processing overhead is <code class="highlighter-rouge">O(1)</code>. Here, we use only 12 bytes, 27% of the original codec.</p>

<p>There are other ways to futher reduce the size if we aren’t storing the whole order book. For example, contiguous tick prices tend to be close to each other, which means <a href="https://gist.github.com/mfuerstenau/ba870a29e16536fdbaba">zigzag encoding</a> can be useful. If the price only goes upward(possible with a hypothetical ponzi scheme based funds), we can use <a href="https://developers.google.com/protocol-buffers/docs/encoding">varint</a>. Also, doing delta encoding over the bytes could potentially save a few bytes, but would cost O(n) in additional processing time, same with run length encoding: first run a decompression over bytes, then decompress again.</p>

<h3 id="metadata">Metadata</h3>

<p>We also need to store metadata about the dtf file in the header.</p>

<p><code class="highlighter-rouge">
Offset 00: ([u8; 5]) magic value 0x4454469001 (DTF9001)
Offset 05: ([u8; 20]) Symbol
Offset 25: (u64) number of records
Offset 33: (u32) max ts
Offset 80: -- records - see below --
</code></p>

<p>First put a magic value to identify the kind of file. Then store the symbol and exchange for the file in 20 characters. Then number of records and maximum timestamp so during decoding, it is unnecessary to read the entire file to find this information.</p>

<p>When dealing with large amount of structured data, it makes sense to build a simple binary file format to increase storage and processing efficiency.</p>

<p>If you want to learn more about file encodings, take a look at the <a href="https://github.com/sripathikrishnan/redis-rdb-tools/wiki/Redis-RDB-Dump-File-Format">spec for redis files</a>.</p>

<h3 id="read-write-traits">Read, Write traits</h3>

<p>The above encoder/decoder can support a multitude of buffers: BufWriter, TcpStream, String, etc… thanks to the trait system in Rust. As long as the buffer struct implements <code class="highlighter-rouge">Read</code>, <code class="highlighter-rouge">Write</code> traits, it can used to transfer or store orderbook ticks. This saves a lot of boilerplate.</p>

<p><code class="highlighter-rouge">rust
fn write_magic_value(wtr: &amp;mut Write) {
    let _ = wtr.write(MAGIC_VALUE);
}
</code></p>

<h2 id="designing-a-tcp-server">Designing a TCP Server</h2>

<p>Now that a format is in place, it’s time to build a TCP server and start serving requests.</p>

<h3 id="design">Design</h3>

<p>We break the problem down into these subproblems:</p>

<ul>
  <li>
    <p>a shared state that is thread-safe</p>
  </li>
  <li>
    <p>threadpool for capping the number of connections</p>
  </li>
  <li>
    <p>each client needs its own thread and a separate state</p>
  </li>
</ul>

<h3 id="threading">Threading</h3>

<p>This is my first naive implementation: spawn a separate thread when a new client connects.</p>

<p>```rust
let listener = match TcpListener::bind(&amp;addr) {
    Ok(listner) =&gt; listener,
    Err(e) =&gt; panic!(format!(“”, e.description()))
};</p>

<p>for stream in listener.incoming() {
    let stream = stream.unwrap();
    thread.spawn(move || {
        let mut buf = [0; 2048];
        loop {
            let bytes_read = stream.read(&amp;mut buf).unwrap();
            if bytes_read == 0 { break }
            let req = str::from_utf8(&amp;buf[..(bytes_read-1)]).unwrap();
            for line in req.split(‘\n’) {
                stream.write(“Response”.as_bytes()).unwrap()
            }
        }
    })
}
```</p>

<p>However, when a client goes haywire and opens up millions of connections, the server will eventually run out of memory and segfault. To cap the number of connections, use a threadpool.</p>

<h3 id="threadpool">Threadpool</h3>

<p>The threadpool implementation is covered in the Rust book. It is interesting because it covers pretty much all grounds of Rust syntax and idiosyncrasies.</p>

<p>```rust
use std::thread;
use std::sync::{mpsc, Arc, Mutex};</p>

<p>enum Message {
    NewJob(Job),
    Terminate,
}</p>

<p>pub struct ThreadPool {
    workers: Vec<worker>,
    sender: mpsc::Sender<message>,
}</message></worker></p>

<p>trait FnBox {
    fn call_box(self: Box<self>);
}</self></p>

<p>impl&lt;F: FnOnce()&gt; FnBox for F {
    fn call_box(self: Box<f>) {
        (*self)()
    }
}</f></p>

<p>type Job = Box&lt;FnBox + Send + ‘static&gt;;</p>

<p>impl ThreadPool {
    pub fn new(size: usize) -&gt; ThreadPool {
        assert!(size &gt; 0);
        let (sender, receiver) = mpsc::channel();
        let receiver = Arc::new(Mutex::new(receiver));
        let mut workers = Vec::with_capacity(size);
        for id in 0..size {
            workers.push(Worker::new(id, receiver.clone()));
        }
        ThreadPool {
            workers,
            sender,
        }
    }
    pub fn execute<f>(&amp;self, f: F)
        where
            F: FnOnce() + Send + 'static
    {
        let job = Box::new(f);</f></p>

<div class="highlighter-rouge"><pre class="highlight"><code>    self.sender.send(Message::NewJob(job)).unwrap();
} }
</code></pre>
</div>

<p>impl Drop for ThreadPool {
    fn drop(&amp;mut self) {
        for _ in &amp;mut self.workers {
            self.sender.send(Message::Terminate).unwrap();
        }
        for worker in &amp;mut self.workers {
            if let Some(thread) = worker.thread.take() {
                thread.join().unwrap();
            }
        }
    }
}
struct Worker {
    id: usize,
    thread: Option&lt;thread::JoinHandle&lt;()»,
}
impl Worker {
    fn new(id: usize, receiver: Arc&lt;Mutex&lt;mpsc::Receiver<message>&gt;&gt;) -&gt;
        Worker {
        let thread = thread::spawn(move ||{
            loop {
                let message = receiver.lock().unwrap().recv().unwrap();
                match message {
                    Message::NewJob(job) =&gt; {
                        job.call_box();
                    },
                    Message::Terminate =&gt; {
                        break;
                    },
                }
            }
        });
        Worker {
            id,
            thread: Some(thread),
        }
    }
}
```</message></p>

<p>The implementation is straightforward, we store the workers in a Vec. As long as the workers are not exhausted, we assign an FnOnce closure and the worker does job exactly once.</p>

<p>Arc stands for atomic reference counting. Because Rc is not designed for use in multiple threads, it is not safe to be shared between threads. Arc can be shared. In Rust, there are two important traits to ensure thread safety: <code class="highlighter-rouge">Send</code> and <code class="highlighter-rouge">Sync</code>. The two traits usually appears in pair. <code class="highlighter-rouge">Send</code> means you can send the data to threads safely without trigger a deepcopy, and the compiler will implement Send for you when deemed fit. Basically, <code class="highlighter-rouge">Arc::clone(&amp;locked_obj)</code> creates an atomic reference that can be sent(as in <code class="highlighter-rouge">mpsc</code>) or move to another thread(as in <code class="highlighter-rouge">move</code> closure). By clone an Arc, the reference count is incremented. When the clone is dropped, the counter is decremented. When all the references across all threads are destroyed, such that <code class="highlighter-rouge">count == 0</code>, then the chunk of memory is released. <code class="highlighter-rouge">Sync</code> means every modification to the data will be synchronized between the threads, it is what <code class="highlighter-rouge">Mutex</code> and <code class="highlighter-rouge">RwLock</code> are for.</p>

<p>To use threadpool, simply replace <code class="highlighter-rouge">thread::spawn</code> with 
<code class="highlighter-rouge">rust
let pool = ThreadPool::new(settings.threads);
for stream in listener.incoming() {
    let stream = stream.unwrap();
    pool.execute(move || {
        handle(&amp;stream);
    });
}
</code></p>

<h3 id="shared-state">Shared state</h3>

<p>Next we define a global state that allows a number of readers but only one writer can acquire the lock.</p>

<p>```rust
struct SharedState {
    pub connections: u16,
    pub settings: Settings,
    pub vec_store: HashMap&lt;String, VecStore&gt;,
    pub history: History,
}</p>

<p>impl SharedState {
    pub fn new(settings: Settings) -&gt; SharedState {
        let mut hashmap = HashMap::new();
        hashmap.insert(“default”.to_owned(), (Vec::new(),0) );
        SharedState {
            connections: 0,
            settings,
            vec_store: hashmap,
            history: HashMap::new(),
        }
    }
}</p>

<p>let global = Arc::new(RwLock::new(SharedState::new(settings.clone())));</p>

<p>for stream in listener.incoming() {
    let stream = stream.unwrap();
    let global_copy = global.clone();
    pool.execute(move || {
        on_connect(&amp;global_copy);
        handle_client(stream, &amp;global_copy);
        on_disconnect(&amp;global_copy);
    });
}</p>

<p>fn on_connect(global: &amp;LockedGlobal) {
    {
        let mut glb_wtr = global.write().unwrap();
        glb_wtr.connections += 1;
    }</p>

<div class="highlighter-rouge"><pre class="highlight"><code>info!("Client connected. Current: {}.", global.read().unwrap().connections); }
</code></pre>
</div>

<p>fn on_disconnect(global: &amp;LockedGlobal) {
    {
        let mut glb_wtr = global.write().unwrap();
        glb_wtr.connections -= 1;
    }</p>

<div class="highlighter-rouge"><pre class="highlight"><code>let rdr = global.read().unwrap();
info!("Client connection disconnected. Current: {}.", rdr.connections); } ```
</code></pre>
</div>

<p><code class="highlighter-rouge">SharedState</code> is a state that all children processes have access to. <code class="highlighter-rouge">connections</code> is a count of how many clients are connected, <code class="highlighter-rouge">settings</code> is a shared copy of the setting, <code class="highlighter-rouge">vec_store</code> stores the Updates and <code class="highlighter-rouge">history</code> records usage statistics.</p>

<p>As a demonstration of modifying a mutable shared state, here is duplication of the code to append to <code class="highlighter-rouge">history</code>. Spawn another thread that executes every x seconds depends on the desired granularity.</p>

<p>```rust
// Timer for recording history
{
    let global_copy_timer = global.clone();
    let granularity = settings.hist_granularity.clone();
    thread::spawn(move || {
        let dur = time::Duration::from_secs(granularity);
        loop {
            {
                let mut rwdr = global_copy_timer.write().unwrap();
                let (total, sizes) = {
                    let mut total = 0;
                    let mut sizes: Vec&lt;(String, u64)&gt; = Vec::new();
                    for (name, vec) in rwdr.vec_store.iter() {
                        let size = vec.1;
                        total += size;
                        sizes.push((name.clone(), size));
                    }
                    sizes.push((“total”.to_owned(), total));
                    (total, sizes)
                };</p>

<div class="highlighter-rouge"><pre class="highlight"><code>            let current_t = time::SystemTime::now();
            for &amp;(ref name, size) in sizes.iter() {
                if !rwdr.history.contains_key(name) {
                    rwdr.history.insert(name.clone(), Vec::new());
                }
                rwdr.history.get_mut(name)
                            .unwrap()
                            .push((current_t, size));
            }

            info!("Current total count: {}", total);
        }

        thread::sleep(dur);
    }
}); } ```
</code></pre>
</div>

<p>This piece of code is pretty bizarre at a first glance due to the usage of scopes. Rust has a particular take on scoping: especially with Locks. Basically, obtaining a lock</p>

<p><code class="highlighter-rouge">let mut rwdr = global_copy_timer.write().unwrap();</code></p>

<p>returns a <code class="highlighter-rouge">RwLockWriteGuard</code>. When it goes out scope(<code class="highlighter-rouge">Drop</code>ped), the exclusive write access is released. This means we must have read lock and write lock in separate scopes, or else we’ll end up in a deadlock. In this case, since the write lock should be released when the thread is sleeping, it is dropped explicitly.</p>

<h3 id="counting">Counting</h3>

<p>The idea is to automatically flush to disk on every 10k inserts and clear the memory. This is done by recording two sizes: a nominal count: rows in memory plus rows in files; and an actual count of rows in memory. When the rows in memory exceeds a threshold, append to the file and clear memory. Then, when another client needs historical data, the server can load data from file without affecting the count.</p>

<p>Here is the implementation of auto flush:</p>

<p>```rust
pub fn add(&amp;mut self, new_vec : dtf::Update) {
    let is_autoflush = {
        let mut wtr = self.global.write().unwrap();
        let is_autoflush = wtr.settings.autoflush;
        let flush_interval = wtr.settings.flush_interval;
        let folder = wtr.settings.dtf_folder.to_owned();
        let vecs = wtr.vec_store.get_mut(&amp;self.name).expect(“KEY IS NOT IN HASHMAP”);</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    vecs.0.push(new_vec);
    vecs.1 += 1;

    // Flush current store into disk after n items is inserted.
    let size = vecs.0.len();
    let is_autoflush = is_autoflush
                    &amp;&amp; size != 0
                    &amp;&amp; (size as u32) % flush_interval == 0;
    if is_autoflush {
        debug!("AUTOFLUSHING {}! Size: {} Last: {:?}", self.name, size, vecs.0.last().clone().unwrap());
    }
    is_autoflush
};
if is_autoflush {
    self.flush();
} } ```
</code></pre>
</div>

<p><code class="highlighter-rouge">self</code> is borrowed in the first line so we have to manually create a scope to drop <code class="highlighter-rouge">self</code>. This piece of code is a good (bad?) example of how Rust developers fight the compiler. This makes memory management a LOT cleaner and way more tractable.</p>

<h3 id="command-parser">Command Parser</h3>

<p>The command parser is still rudimentary as it is implemented using a series of match and if-else statements similar to Redis. However, it gets the job done. I will refactor this should demand arise.</p>

<p>Here is a list of commands:</p>

<ul>
  <li>PING</li>
  <li>HELP</li>
  <li>INFO</li>
  <li>PERF</li>
  <li>COUNT</li>
  <li>COUNT ALL</li>
  <li>CREATE [db]</li>
  <li>USE [db]</li>
  <li>ADD [row]</li>
  <li>ADD [row] INTO [db]</li>
  <li>BULKADD</li>
  <li>BULKADD INTO [db]</li>
  <li>CLEAR</li>
  <li>CLEAR ALL</li>
  <li>FLUSH</li>
  <li>FLUSH ALL</li>
  <li>GET [count]</li>
  <li>GET [count] AS JSON</li>
  <li>GET ALL</li>
  <li>GET ALL AS JSON</li>
</ul>

<p>If not specified <code class="highlighter-rouge">AS JSON</code>, it uses the above serialization format.</p>

<h2 id="tools">Tools</h2>

<p><code class="highlighter-rouge">dtfcat</code> is a simple reader for <code class="highlighter-rouge">.dtf</code> files. It can read metadata, convert dtf to csv, rebin and split into smaller files by time buckets.</p>

<h2 id="client-implementations">Client implementations</h2>

<p>I had some trouble writing a client in JavaScript. Here is an implementation in TypeScript:</p>

<p>```typescript
const net = require(‘net’);
const THREADS = 20;
const PORT = 9001;
const HOST = ‘localhost’;</p>

<p>import { DBUpdate } from ‘../typings’;</p>

<p>interface TectonicResponse {
    success: boolean;
    data: string;
}</p>

<p>type SocketMsgCb = (res: TectonicResponse) =&gt; void;</p>

<p>export interface SocketQuery {
    message: string;
    cb: SocketMsgCb;
    onError: (err: any) =&gt; void;
}</p>

<p>export default class TectonicDB {
    port : number;
    address : string;
    socket: any;
    initialized: boolean;
    dead: boolean;
    private onDisconnect: any;</p>

<div class="highlighter-rouge"><pre class="highlight"><code>private socketSendQueue: SocketQuery[];
private activeQuery?: SocketQuery;
private readerBuffer: Buffer;

// tslint:disable-next-line:no-empty
constructor(port=PORT, address=HOST, onDisconnect=((queue: SocketQuery[]) =&gt; { })) {
    this.socket = new net.Socket();
    this.activeQuery = null;
    this.address = address || HOST;
    this.port = port || PORT;
    this.initialized = false;
    this.dead = false;
    this.onDisconnect = onDisconnect;
    this.init();
}

async init() {
    const client = this;

    client.socketSendQueue = [];
    client.readerBuffer = new Buffer([]);

    client.socket.connect(client.port, client.address, () =&gt; {
        // console.log(`Tectonic client connected to: ${client.address}:${client.port}`);
        this.initialized = true;

        // process any queued queries
        if(this.socketSendQueue.length &gt; 0) {
            // console.log('Sending queued message after DB connected...');
            client.activeQuery = this.socketSendQueue.shift();
            client.sendSocketMsg(this.activeQuery.message);
        }
    });

    client.socket.on('close', () =&gt; {
        // console.log('Client closed');
        client.dead = true;
        client.onDisconnect(client.socketSendQueue);
    });

    client.socket.on('data', (data: any) =&gt;
        this.handleSocketData(data));

    client.socket.on('error', (err: any) =&gt; {
        if(client.activeQuery) {
            client.activeQuery.onError(err);
        }
    });
}

// skipped some functions

async bulkadd_into(updates : DBUpdate[], db: string) {
    const ret = [];
    ret.push('BULKADD INTO '+ db);
    for (const { timestamp, seq, is_trade, is_bid, price, size} of updates) {
        ret.push(`${timestamp}, ${seq}, ${is_trade ? 't' : 'f'}, ${is_bid ? 't':'f'}, ${price}, ${size};`);
    }
    ret.push('DDAKLUB');
    this.cmd(ret);
}

async use(dbname: string) {
    return this.cmd(`USE ${dbname}`);
}

handleSocketData(data: Buffer) {
    const client = this;

    const totalLength = client.readerBuffer.length + data.length;
    client.readerBuffer = Buffer.concat([client.readerBuffer, data], totalLength);

    // check if received a full response from stream, if no, store to buffer.
    const firstResponse = client.readerBuffer.indexOf(0x0a); // chr(0x0a) == '\n'
    if (firstResponse === -1) { // newline not found
        return;
    } else {
        // data up to first newline
        const data = client.readerBuffer.subarray(0, firstResponse+1);
        // remove up to first newline
        const rest = client.readerBuffer.subarray(firstResponse+1, client.readerBuffer.length);
        client.readerBuffer = new Buffer(rest);

        const success = data.subarray(0, 8)[0] === 1;
        const len = new Uint32Array(data.subarray(8,9))[0];
        const dataBody : string = String.fromCharCode.apply(null, data.subarray(9, 12+len));
        const response : TectonicResponse = {success, data: dataBody};

        if (client.activeQuery) {
            // execute the stored callback with the result of the query, fulfilling the promise
            client.activeQuery.cb(response);
        }

        // if there's something left in the queue to process, do it next
        // otherwise set the current query to empty
        if(client.socketSendQueue.length === 0) {
            client.activeQuery = null;
        } else {
            // equivalent to `popFront()`
            client.activeQuery = this.socketSendQueue.shift();
            client.sendSocketMsg(client.activeQuery.message);
        }
    }
}

sendSocketMsg(msg: string) {
    this.socket.write(msg+'\n');
}

cmd(message: string | string[]) : Promise&lt;TectonicResponse&gt; {
    const client = this;
    let ret: Promise&lt;TectonicResponse&gt;;

    if (Array.isArray(message)) {
         ret = new Promise((resolve, reject) =&gt; {
            for (const m of message) {
                client.socketSendQueue.push({
                    message: m,
                    cb: m === 'DDAKLUB' ? resolve : () =&gt; {},
                    onError: reject,
                });
            }
        });
    } else if (typeof message === 'string') {
        ret = new Promise((resolve, reject) =&gt; {
            const query: SocketQuery = {
                message,
                cb: resolve,
                onError: reject,
            };
            client.socketSendQueue.push(query);
        });
    }

    if (client.activeQuery == null &amp;&amp; this.initialized) {
        client.activeQuery = this.socketSendQueue.shift();
        client.sendSocketMsg(client.activeQuery.message);
    }

    return ret;
}

exit() {
    this.socket.destroy();
}

getQueueLen(): number {
    return this.socketSendQueue.length;
}

concatQueue(otherQueue: SocketQuery[]) {
    this.socketSendQueue = this.socketSendQueue
                            .concat(otherQueue);
} } ```
</code></pre>
</div>

<p>It uses a FIFO queue to keep the db calls in order.</p>

<p>There is also a connection pool class for distributing loads. It was quite an undertaking.</p>

<h1 id="conclusion">Conclusion</h1>

<p>You’ve made it this far, congrats! Here is a screenshot of tectonic running in semi-production:</p>

<p><img src="https://i.imgur.com/PttCo1v.png" alt="TectonicDB in action" /></p>

<p>It inserts ~100k records every 30 seconds. or 3000 inserts per second. Hope this post is helpful to your own development.</p>

<p>I am pretty happy with the end result. The database compiles to a 4mb binary executable and can handle millions of inserts per second. The bottleneck was always the client.</p>

<p>Although Rust is not the fastest language to prototype with, the compiler improves the quality of life drastically.</p>

<h1 id="improvement">Improvement</h1>

<ul>
  <li>
    <p>Python client.</p>
  </li>
  <li>
    <p>Sharding</p>
  </li>
  <li>
    <p>Event dispatch. Similar to PostgreSQL’s event system.</p>
  </li>
  <li>
    <p>Master-slave replication</p>
  </li>
</ul>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Ricky Han blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Ricky Han blog</li>
          <li><a href="mailto:rickyhan+blog@rickyhan.com">rickyhan+blog@rickyhan.com</a></li>
          <li><a href="https://tinyletter.com/rickyhan">Subscribe</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Programming demos, tips, thoughts.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
